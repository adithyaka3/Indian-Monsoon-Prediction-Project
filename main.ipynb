{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104e7854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gridding import griddata\n",
    "from singleAutoencoder import getEncodeddata, testGetEncodeddata\n",
    "from doubleAutoencoder import getEncodeddataDouble\n",
    "from preprocessAllIndiaRainfall import get_jjas_rainfall\n",
    "from rankedPredictors import getRankedPredictors, testGetRankedPredictors\n",
    "from predict import prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef382b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param = \"mslp\"  # Example parameter, can be changed as needed\n",
    "#First step is to grid the data into bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2eeb5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridding the data...\n",
      "Gridding completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Gridding the data...\")\n",
    "griddata(param = param, level = 200, gridsize= 10, starttime = \"1980-01-01\", endtime = \"2010-12-31\")\n",
    "print(\"Gridding completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf500e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rainfall data...\n",
      "JJAS rainfall data saved to torch_objects/train_annual_jjas_rainfall_data_south_peninsular.pt\n",
      "Rainfall data processed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#second step if to process the rainfall data. \n",
    "# For all india rainfall, we need to get the JJAS rainfall for all years\n",
    "print(\"Processing rainfall data...\")\n",
    "rain_file_tensor = get_jjas_rainfall(start_year=1980, end_year=2010, test = False) # Inclusive of the end year, i.e 31-12-end_year\n",
    "print(\"Rainfall data processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c0542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the data using single autoencoder...\n",
      "Epoch 1/200, Loss: 0.1833\n",
      "Epoch 2/200, Loss: 0.0320\n",
      "Epoch 3/200, Loss: 0.0087\n",
      "Epoch 4/200, Loss: 0.0065\n",
      "Epoch 5/200, Loss: 0.0052\n",
      "Epoch 6/200, Loss: 0.0049\n",
      "Epoch 7/200, Loss: 0.0048\n",
      "Epoch 8/200, Loss: 0.0048\n",
      "Epoch 9/200, Loss: 0.0048\n",
      "Epoch 10/200, Loss: 0.0048\n",
      "Epoch 11/200, Loss: 0.0048\n",
      "Epoch 12/200, Loss: 0.0048\n",
      "Epoch 13/200, Loss: 0.0048\n",
      "Epoch 14/200, Loss: 0.0048\n",
      "Epoch 15/200, Loss: 0.0048\n",
      "Epoch 16/200, Loss: 0.0048\n",
      "Epoch 17/200, Loss: 0.0048\n",
      "Epoch 18/200, Loss: 0.0048\n",
      "Epoch 19/200, Loss: 0.0048\n",
      "Epoch 20/200, Loss: 0.0048\n",
      "Epoch 21/200, Loss: 0.0048\n",
      "Epoch 22/200, Loss: 0.0048\n",
      "Epoch 23/200, Loss: 0.0048\n",
      "Epoch 24/200, Loss: 0.0048\n",
      "Epoch 25/200, Loss: 0.0048\n",
      "Epoch 26/200, Loss: 0.0048\n",
      "Epoch 27/200, Loss: 0.0048\n",
      "Epoch 28/200, Loss: 0.0048\n",
      "Epoch 29/200, Loss: 0.0048\n",
      "Epoch 30/200, Loss: 0.0048\n",
      "Epoch 31/200, Loss: 0.0048\n",
      "Epoch 32/200, Loss: 0.0048\n",
      "Epoch 33/200, Loss: 0.0048\n",
      "Epoch 34/200, Loss: 0.0048\n",
      "Epoch 35/200, Loss: 0.0048\n",
      "Epoch 36/200, Loss: 0.0048\n",
      "Epoch 37/200, Loss: 0.0048\n",
      "Epoch 38/200, Loss: 0.0048\n",
      "Epoch 39/200, Loss: 0.0048\n",
      "Epoch 40/200, Loss: 0.0048\n",
      "Epoch 41/200, Loss: 0.0048\n",
      "Epoch 42/200, Loss: 0.0048\n",
      "Epoch 43/200, Loss: 0.0047\n",
      "Epoch 44/200, Loss: 0.0047\n",
      "Epoch 45/200, Loss: 0.0047\n",
      "Epoch 46/200, Loss: 0.0046\n",
      "Epoch 47/200, Loss: 0.0046\n",
      "Epoch 48/200, Loss: 0.0045\n",
      "Epoch 49/200, Loss: 0.0044\n",
      "Epoch 50/200, Loss: 0.0043\n",
      "Epoch 51/200, Loss: 0.0041\n",
      "Epoch 52/200, Loss: 0.0040\n",
      "Epoch 53/200, Loss: 0.0039\n",
      "Epoch 54/200, Loss: 0.0038\n",
      "Epoch 55/200, Loss: 0.0037\n",
      "Epoch 56/200, Loss: 0.0037\n",
      "Epoch 57/200, Loss: 0.0037\n",
      "Epoch 58/200, Loss: 0.0036\n",
      "Epoch 59/200, Loss: 0.0036\n",
      "Epoch 60/200, Loss: 0.0035\n",
      "Epoch 61/200, Loss: 0.0034\n",
      "Epoch 62/200, Loss: 0.0032\n",
      "Epoch 63/200, Loss: 0.0031\n",
      "Epoch 64/200, Loss: 0.0030\n",
      "Epoch 65/200, Loss: 0.0030\n",
      "Epoch 66/200, Loss: 0.0029\n",
      "Epoch 67/200, Loss: 0.0029\n",
      "Epoch 68/200, Loss: 0.0029\n",
      "Epoch 69/200, Loss: 0.0028\n",
      "Epoch 70/200, Loss: 0.0028\n",
      "Epoch 71/200, Loss: 0.0028\n",
      "Epoch 72/200, Loss: 0.0028\n",
      "Epoch 73/200, Loss: 0.0028\n",
      "Epoch 74/200, Loss: 0.0028\n",
      "Epoch 75/200, Loss: 0.0028\n",
      "Epoch 76/200, Loss: 0.0028\n",
      "Epoch 77/200, Loss: 0.0028\n",
      "Epoch 78/200, Loss: 0.0028\n",
      "Epoch 79/200, Loss: 0.0028\n",
      "Epoch 80/200, Loss: 0.0028\n",
      "Epoch 81/200, Loss: 0.0028\n",
      "Epoch 82/200, Loss: 0.0028\n",
      "Epoch 83/200, Loss: 0.0028\n",
      "Epoch 84/200, Loss: 0.0028\n",
      "Epoch 85/200, Loss: 0.0028\n",
      "Epoch 86/200, Loss: 0.0028\n",
      "Epoch 87/200, Loss: 0.0028\n",
      "Epoch 88/200, Loss: 0.0028\n",
      "Epoch 89/200, Loss: 0.0028\n",
      "Epoch 90/200, Loss: 0.0028\n",
      "Epoch 91/200, Loss: 0.0028\n",
      "Epoch 92/200, Loss: 0.0028\n",
      "Epoch 93/200, Loss: 0.0028\n",
      "Epoch 94/200, Loss: 0.0028\n",
      "Epoch 95/200, Loss: 0.0028\n",
      "Epoch 96/200, Loss: 0.0028\n",
      "Epoch 97/200, Loss: 0.0028\n",
      "Epoch 98/200, Loss: 0.0028\n",
      "Epoch 99/200, Loss: 0.0028\n",
      "Epoch 100/200, Loss: 0.0028\n",
      "Epoch 101/200, Loss: 0.0028\n",
      "Epoch 102/200, Loss: 0.0028\n",
      "Epoch 103/200, Loss: 0.0028\n",
      "Epoch 104/200, Loss: 0.0028\n",
      "Epoch 105/200, Loss: 0.0028\n",
      "Epoch 106/200, Loss: 0.0028\n",
      "Epoch 107/200, Loss: 0.0028\n",
      "Epoch 108/200, Loss: 0.0028\n",
      "Epoch 109/200, Loss: 0.0028\n",
      "Epoch 110/200, Loss: 0.0028\n",
      "Epoch 111/200, Loss: 0.0028\n",
      "Epoch 112/200, Loss: 0.0028\n",
      "Epoch 113/200, Loss: 0.0028\n",
      "Epoch 114/200, Loss: 0.0028\n",
      "Epoch 115/200, Loss: 0.0028\n",
      "Epoch 116/200, Loss: 0.0028\n",
      "Epoch 117/200, Loss: 0.0028\n",
      "Epoch 118/200, Loss: 0.0028\n",
      "Epoch 119/200, Loss: 0.0028\n",
      "Epoch 120/200, Loss: 0.0028\n",
      "Epoch 121/200, Loss: 0.0028\n",
      "Epoch 122/200, Loss: 0.0028\n",
      "Epoch 123/200, Loss: 0.0028\n",
      "Epoch 124/200, Loss: 0.0028\n",
      "Epoch 125/200, Loss: 0.0028\n",
      "Epoch 126/200, Loss: 0.0028\n",
      "Epoch 127/200, Loss: 0.0028\n",
      "Epoch 128/200, Loss: 0.0028\n",
      "Epoch 129/200, Loss: 0.0028\n",
      "Epoch 130/200, Loss: 0.0028\n",
      "Epoch 131/200, Loss: 0.0028\n",
      "Epoch 132/200, Loss: 0.0028\n",
      "Epoch 133/200, Loss: 0.0028\n",
      "Epoch 134/200, Loss: 0.0027\n",
      "Epoch 135/200, Loss: 0.0027\n",
      "Epoch 136/200, Loss: 0.0027\n",
      "Epoch 137/200, Loss: 0.0027\n",
      "Epoch 138/200, Loss: 0.0027\n",
      "Epoch 139/200, Loss: 0.0027\n",
      "Epoch 140/200, Loss: 0.0027\n",
      "Epoch 141/200, Loss: 0.0026\n",
      "Epoch 142/200, Loss: 0.0026\n",
      "Epoch 143/200, Loss: 0.0026\n",
      "Epoch 144/200, Loss: 0.0026\n",
      "Epoch 145/200, Loss: 0.0026\n",
      "Epoch 146/200, Loss: 0.0026\n",
      "Epoch 147/200, Loss: 0.0026\n",
      "Epoch 148/200, Loss: 0.0026\n",
      "Epoch 149/200, Loss: 0.0025\n",
      "Epoch 150/200, Loss: 0.0025\n",
      "Epoch 151/200, Loss: 0.0025\n",
      "Epoch 152/200, Loss: 0.0025\n",
      "Epoch 153/200, Loss: 0.0025\n",
      "Epoch 154/200, Loss: 0.0025\n",
      "Epoch 155/200, Loss: 0.0025\n",
      "Epoch 156/200, Loss: 0.0025\n",
      "Epoch 157/200, Loss: 0.0025\n",
      "Epoch 158/200, Loss: 0.0025\n",
      "Epoch 159/200, Loss: 0.0024\n",
      "Epoch 160/200, Loss: 0.0024\n",
      "Epoch 161/200, Loss: 0.0024\n",
      "Epoch 162/200, Loss: 0.0024\n",
      "Epoch 163/200, Loss: 0.0024\n",
      "Epoch 164/200, Loss: 0.0024\n",
      "Epoch 165/200, Loss: 0.0024\n",
      "Epoch 166/200, Loss: 0.0024\n",
      "Epoch 167/200, Loss: 0.0024\n",
      "Epoch 168/200, Loss: 0.0024\n",
      "Epoch 169/200, Loss: 0.0024\n",
      "Epoch 170/200, Loss: 0.0024\n",
      "Epoch 171/200, Loss: 0.0024\n",
      "Epoch 172/200, Loss: 0.0024\n",
      "Epoch 173/200, Loss: 0.0024\n",
      "Epoch 174/200, Loss: 0.0024\n",
      "Epoch 175/200, Loss: 0.0024\n",
      "Epoch 176/200, Loss: 0.0024\n",
      "Epoch 177/200, Loss: 0.0023\n",
      "Epoch 178/200, Loss: 0.0023\n",
      "Epoch 179/200, Loss: 0.0023\n",
      "Epoch 180/200, Loss: 0.0023\n",
      "Epoch 181/200, Loss: 0.0023\n",
      "Epoch 182/200, Loss: 0.0023\n",
      "Epoch 183/200, Loss: 0.0023\n",
      "Epoch 184/200, Loss: 0.0023\n",
      "Epoch 185/200, Loss: 0.0022\n",
      "Epoch 186/200, Loss: 0.0022\n",
      "Epoch 187/200, Loss: 0.0022\n",
      "Epoch 188/200, Loss: 0.0022\n",
      "Epoch 189/200, Loss: 0.0022\n",
      "Epoch 190/200, Loss: 0.0022\n",
      "Epoch 191/200, Loss: 0.0022\n",
      "Epoch 192/200, Loss: 0.0022\n",
      "Epoch 193/200, Loss: 0.0021\n",
      "Epoch 194/200, Loss: 0.0022\n",
      "Epoch 195/200, Loss: 0.0021\n",
      "Epoch 196/200, Loss: 0.0021\n",
      "Epoch 197/200, Loss: 0.0021\n",
      "Epoch 198/200, Loss: 0.0021\n",
      "Epoch 199/200, Loss: 0.0021\n",
      "Epoch 200/200, Loss: 0.0021\n",
      "Encoding completed using single autoencoder.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Next step is to encode the data using the autoencoder\n",
    "print(\"Encoding the data using single autoencoder...\")\n",
    "W1, b1, W2, b2, W3, b3 = getEncodeddata(variable_name=param, epochs=200)\n",
    "print(\"Encoding completed using single autoencoder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be256bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ranked predictors...\n",
      "--- Processing variable: mslp ---\n",
      "--- Top 4 predictors for variable mslp and file encoded_h1_mslp.pt---\n",
      "Top Correlations: [0.48922136425971985, 0.48129332065582275, 0.47964245080947876, 0.4795280694961548]\n",
      "\n",
      "--- Top 5 predictors for variable mslp and file encoded_h1_mslp.pt---\n",
      "Top Correlations: [0.48922136425971985, 0.48129332065582275, 0.47964245080947876, 0.4795280694961548, 0.47321465611457825]\n",
      "\n",
      "--- Top 6 predictors for variable mslp and file encoded_h1_mslp.pt---\n",
      "Top Correlations: [0.48922136425971985, 0.48129332065582275, 0.47964245080947876, 0.4795280694961548, 0.47321465611457825, 0.42949604988098145]\n",
      "\n",
      "--- Top 8 predictors for variable mslp and file encoded_h1_mslp.pt---\n",
      "Top Correlations: [0.48922136425971985, 0.48129332065582275, 0.47964245080947876, 0.4795280694961548, 0.47321465611457825, 0.42949604988098145, -0.42862218618392944, 0.41827526688575745]\n",
      "\n",
      "--- Top 10 predictors for variable mslp and file encoded_h1_mslp.pt---\n",
      "Top Correlations: [0.48922136425971985, 0.48129332065582275, 0.47964245080947876, 0.4795280694961548, 0.47321465611457825, 0.42949604988098145, -0.42862218618392944, 0.41827526688575745, 0.4086925685405731, 0.40671852231025696]\n",
      "\n",
      "--- Top 4 predictors for variable mslp and file encoded_h2_mslp.pt---\n",
      "Top Correlations: [0.429698646068573, 0.41991737484931946, 0.4066706895828247, 0.3900376558303833]\n",
      "\n",
      "--- Top 5 predictors for variable mslp and file encoded_h2_mslp.pt---\n",
      "Top Correlations: [0.429698646068573, 0.41991737484931946, 0.4066706895828247, 0.3900376558303833, 0.386725515127182]\n",
      "\n",
      "--- Top 6 predictors for variable mslp and file encoded_h2_mslp.pt---\n",
      "Top Correlations: [0.429698646068573, 0.41991737484931946, 0.4066706895828247, 0.3900376558303833, 0.386725515127182, 0.385143905878067]\n",
      "\n",
      "--- Top 8 predictors for variable mslp and file encoded_h2_mslp.pt---\n",
      "Top Correlations: [0.429698646068573, 0.41991737484931946, 0.4066706895828247, 0.3900376558303833, 0.386725515127182, 0.385143905878067, 0.3759278953075409, 0.37126055359840393]\n",
      "\n",
      "--- Top 10 predictors for variable mslp and file encoded_h2_mslp.pt---\n",
      "Top Correlations: [0.429698646068573, 0.41991737484931946, 0.4066706895828247, 0.3900376558303833, 0.386725515127182, 0.385143905878067, 0.3759278953075409, 0.37126055359840393, 0.368554949760437, 0.3599310517311096]\n",
      "\n",
      "--- Top 4 predictors for variable mslp and file encoded_h3_mslp.pt---\n",
      "Top Correlations: [0.37126442790031433, -0.30782750248908997, 0.29322969913482666, 0.28749409317970276]\n",
      "\n",
      "--- Top 5 predictors for variable mslp and file encoded_h3_mslp.pt---\n",
      "Top Correlations: [0.37126442790031433, -0.30782750248908997, 0.29322969913482666, 0.28749409317970276, 0.2782066762447357]\n",
      "\n",
      "--- Top 6 predictors for variable mslp and file encoded_h3_mslp.pt---\n",
      "Top Correlations: [0.37126442790031433, -0.30782750248908997, 0.29322969913482666, 0.28749409317970276, 0.2782066762447357, 0.2709370255470276]\n",
      "\n",
      "--- Top 8 predictors for variable mslp and file encoded_h3_mslp.pt---\n",
      "Top Correlations: [0.37126442790031433, -0.30782750248908997, 0.29322969913482666, 0.28749409317970276, 0.2782066762447357, 0.2709370255470276, -0.2518269121646881, 0.2443884164094925]\n",
      "\n",
      "--- Top 10 predictors for variable mslp and file encoded_h3_mslp.pt---\n",
      "Top Correlations: [0.37126442790031433, -0.30782750248908997, 0.29322969913482666, 0.28749409317970276, 0.2782066762447357, 0.2709370255470276, -0.2518269121646881, 0.2443884164094925, 0.22118264436721802, 0.21908430755138397]\n",
      "\n",
      "Ranked predictors obtained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Next step is to get the ranked predictors\n",
    "print(\"Getting ranked predictors...\")\n",
    "all_correlations = getRankedPredictors(rain_file_tensor, variables=[param])\n",
    "print(\"Ranked predictors obtained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4403ddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test gridding the data...\n",
      "Test gridding completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Test gridding the data...\")\n",
    "griddata(param = param, level = 200, gridsize= 10, starttime = \"2011-01-01\", endtime = \"2015-12-31\")\n",
    "print(\"Test gridding completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bff6aefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rainfall data processing...\n",
      "JJAS rainfall data saved to torch_objects/test_annual_jjas_rainfall_data_south_peninsular.pt\n",
      "Test rainfall data processed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Test rainfall data processing...\")\n",
    "test_rain_file_tensor = get_jjas_rainfall(start_year=2011, end_year=2015, test = True) # Inclusive of the end year, i.e 31-12\n",
    "print(\"Test rainfall data processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1366edef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test encoding the data using single autoencoder...\n",
      "Test encoding completed using single autoencoder.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Test encoding the data using single autoencoder...\")\n",
    "testGetEncodeddata(W1, b1, W2, b2, W3, b3, variable_name=param)\n",
    "print(\"Test encoding completed using single autoencoder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d86fb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ranked predictors...\n",
      "--- Processing variable: mslp ---\n",
      "Test ranked predictors obtained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Test ranked predictors...\")\n",
    "testGetRankedPredictors(all_correlations, variables=[param])\n",
    "print(\"Test ranked predictors obtained.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d35cac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting rainfall using ranked predictors...\n",
      "Predicting using layer 1 and top 4 predictors...\n",
      "Training data shape: (31, 4) (31,)\n",
      "Test data shape: (5, 4) (5,)\n",
      " First 10 predictions: [  2.69632376  -7.55477136 -11.71440657 -13.40666384  11.96886059]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 172.41%\n",
      "Mean Squared Error (MSE): 306.80\n",
      "------------------------------------------------\n",
      "Predicting using layer 1 and top 5 predictors...\n",
      "Training data shape: (31, 5) (31,)\n",
      "Test data shape: (5, 5) (5,)\n",
      " First 10 predictions: [ -0.98917911  -7.0247771   -9.8067606  -14.45359026  11.43441957]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 94.72%\n",
      "Mean Squared Error (MSE): 282.22\n",
      "------------------------------------------------\n",
      "Predicting using layer 1 and top 6 predictors...\n",
      "Training data shape: (31, 6) (31,)\n",
      "Test data shape: (5, 6) (5,)\n",
      " First 10 predictions: [  9.8383229  -10.57020813 -11.43176153 -14.87130469  13.33067063]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 331.96%\n",
      "Mean Squared Error (MSE): 341.97\n",
      "------------------------------------------------\n",
      "Predicting using layer 1 and top 8 predictors...\n",
      "Training data shape: (31, 8) (31,)\n",
      "Test data shape: (5, 8) (5,)\n",
      " First 10 predictions: [  6.18283554  -6.09652292  -8.61475993 -14.27905606  12.35211305]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 251.81%\n",
      "Mean Squared Error (MSE): 292.02\n",
      "------------------------------------------------\n",
      "Predicting using layer 1 and top 10 predictors...\n",
      "Training data shape: (31, 10) (31,)\n",
      "Test data shape: (5, 10) (5,)\n",
      " First 10 predictions: [  8.20609629  -4.16970062 -10.67776325 -14.04810432  12.92435131]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 303.66%\n",
      "Mean Squared Error (MSE): 328.76\n",
      "------------------------------------------------\n",
      "Predicting using layer 2 and top 4 predictors...\n",
      "Training data shape: (31, 4) (31,)\n",
      "Test data shape: (5, 4) (5,)\n",
      " First 10 predictions: [ 14.34257957  -8.3529591  -15.44854554 -15.96797866   1.51321649]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 430.28%\n",
      "Mean Squared Error (MSE): 304.64\n",
      "------------------------------------------------\n",
      "Predicting using layer 2 and top 5 predictors...\n",
      "Training data shape: (31, 5) (31,)\n",
      "Test data shape: (5, 5) (5,)\n",
      " First 10 predictions: [ 15.2069121   -8.35323701 -15.39157173 -15.78038232   2.81193866]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 450.68%\n",
      "Mean Squared Error (MSE): 318.14\n",
      "------------------------------------------------\n",
      "Predicting using layer 2 and top 6 predictors...\n",
      "Training data shape: (31, 6) (31,)\n",
      "Test data shape: (5, 6) (5,)\n",
      " First 10 predictions: [ 12.18758452 -12.79662889 -15.50829828 -15.41185976   9.50176127]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 390.94%\n",
      "Mean Squared Error (MSE): 359.96\n",
      "------------------------------------------------\n",
      "Predicting using layer 2 and top 8 predictors...\n",
      "Training data shape: (31, 8) (31,)\n",
      "Test data shape: (5, 8) (5,)\n",
      " First 10 predictions: [  9.43311507 -10.91399572 -12.55455994 -12.28831215  10.95639678]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 315.25%\n",
      "Mean Squared Error (MSE): 319.41\n",
      "------------------------------------------------\n",
      "Predicting using layer 2 and top 10 predictors...\n",
      "Training data shape: (31, 10) (31,)\n",
      "Test data shape: (5, 10) (5,)\n",
      " First 10 predictions: [ 10.06427241  -8.89045711 -13.4430715  -13.01229488   8.81891079]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 332.12%\n",
      "Mean Squared Error (MSE): 311.85\n",
      "------------------------------------------------\n",
      "Predicting using layer 3 and top 4 predictors...\n",
      "Training data shape: (31, 4) (31,)\n",
      "Test data shape: (5, 4) (5,)\n",
      " First 10 predictions: [-12.5217654  -11.08852963 -12.01900738 -13.14708621  10.25325834]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 345.37%\n",
      "Mean Squared Error (MSE): 313.68\n",
      "------------------------------------------------\n",
      "Predicting using layer 3 and top 5 predictors...\n",
      "Training data shape: (31, 5) (31,)\n",
      "Test data shape: (5, 5) (5,)\n",
      " First 10 predictions: [ -7.78349982 -12.16019097 -13.71432208 -14.40467627   6.18311353]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 241.88%\n",
      "Mean Squared Error (MSE): 279.15\n",
      "------------------------------------------------\n",
      "Predicting using layer 3 and top 6 predictors...\n",
      "Training data shape: (31, 6) (31,)\n",
      "Test data shape: (5, 6) (5,)\n",
      " First 10 predictions: [ -8.63171303 -11.22693415 -12.9753316  -14.35048182   3.95836045]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 255.15%\n",
      "Mean Squared Error (MSE): 254.05\n",
      "------------------------------------------------\n",
      "Predicting using layer 3 and top 8 predictors...\n",
      "Training data shape: (31, 8) (31,)\n",
      "Test data shape: (5, 8) (5,)\n",
      " First 10 predictions: [ -7.40108127  -8.85210403 -11.10881805 -12.96921735  -4.88756907]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 212.10%\n",
      "Mean Squared Error (MSE): 172.90\n",
      "------------------------------------------------\n",
      "Predicting using layer 3 and top 10 predictors...\n",
      "Training data shape: (31, 10) (31,)\n",
      "Test data shape: (5, 10) (5,)\n",
      " First 10 predictions: [ -6.66264667 -14.64757892  -6.24020785 -13.8277136   -0.51337937]\n",
      " First 10 actual values: [ -0.8935746 -10.509622   14.7394495  -7.549769  -15.9290695]\n",
      "Mean Absolute Percentage Error (MAPE): 201.45%\n",
      "Mean Squared Error (MSE): 153.52\n",
      "------------------------------------------------\n",
      "Prediction completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Next step is to predict the rainfall using the ranked predictors\n",
    "print(\"Predicting rainfall using ranked predictors...\")\n",
    "layer = 3\n",
    "top = 5\n",
    "for layer in range(1, 4):\n",
    "    for top in [4, 5, 6, 8, 10]:\n",
    "        print(f\"Predicting using layer {layer} and top {top} predictors...\")\n",
    "        # Load the feature tensors for training and testing\n",
    "        train_feature_data_path = f\"torch_objects/train_features_h{layer}_{param}_top_{top}_predictors.pt\"\n",
    "        test_feature_data_path = f\"torch_objects/test_features_h{layer}_{param}_top_{top}_predictors.pt\"\n",
    "\n",
    "        prediction(rain_file_tensor, test_rain_file_tensor, train_feature_data_path, test_feature_data_path)\n",
    "        print(\"------------------------------------------------\")\n",
    "\n",
    "print(\"Prediction completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
